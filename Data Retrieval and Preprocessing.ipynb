{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Using Fundamental Data to Predict Stock Price\n",
    "### Data Retrieval and Preprocessing\n",
    "\n",
    "**Adam Magyar and Scott Elmore** <br>\n",
    " **DSC 478**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Description:\n",
    "\n",
    "The dataset was put together using a combination of resources. First, we accumulated a list of the top 3000 stocks by market cap (Russell 3000).  Using these stock tickers, we used the 'yfinance' library in python to download the financial info based on quarter for each of the stocks.  Along with financial info, we downloaded the price history for as far back as we necessary in accordance to the financial info available.  This information was saved as .json files so subsequent notebooks could pull in the data by looking up the stock in a dictionary as opposed to re-downloading the data from the API, which takes some time.  During this process, some stocks were removed that didn't have all 4 quarters of necessary data.\n",
    "\n",
    "Next, the dataset was augmented using another python library called 'ta'.  This library returns the outputs of certain technical indicators when passed in the underlying price and volume information.  We had all this information available from the yfinance API.  We decided to append 15 technical indicators that best reflect price and volume movement for each stock at the point of time that their quarterly financial info was released.  We also used another dataset that was downloaded from Bloomberg that contained the adjusted quarterly returns for each stock at each quarter.  For the regression models, we needed an appropriate target feature, so we created a column for 'Next Qtr Return' which simply looked ahead at each stocks quarterly returns and used the next quarter in the future. All this information was coalesced into one dataset 'stock_complete_info.csv'.\n",
    "\n",
    "We decided to augment the dataset one step further by creating TFxIDF bi-grams for each stock's 'business summary'.  Each summary was around 200 words, and with around 2600 stocks, it created a very sparse matrix.  But after creating TFxIDF values for each bi-gram term and seeing the distribution of values, it was clear we could reduce this dataset to the top 500 terms with regards to TFxIDF value. This matrix was appended to the existing data and saved as 'stock_complete_info_bigrams.csv'.\n",
    "\n",
    "Finally, the data was preprocessed to create a friendlier dataset for ML models.  Stocks varied greatly in some of their quarterly fundamental values, such as total market cap, and instead of scaling using Log, we divided each fundamental value by the number of shares each stock had outstanding.  This creates a more 'share-neutral' view of the indicators and allows for better comparison.  Some stocks didn't have values for all fundamental indicators, which we determined meant they should be 0's as opposed to dropping the row completely.  There existed some outliers even after doing a division by shares outstanding, so rows were dropped that exceeded a z-score of 3 in either direction. Finally, dummy variables were created for the 3 features that were text 'country', 'industry', and 'sector'.  This resulted in a final data set titled 'outlier_removed_processed_df_bigrams.csv'."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import json\n",
    "import ta\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# check https://technical-analysis-library-in-python.readthedocs.io/en/latest/ta.html#momentum-indicators for information on ta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download from YFinance API"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ticker_list = pd.read_csv('Russell3000Tickers.csv').values.flatten().tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stock_dict = {}\n",
    "for ticker in ticker_list:\n",
    "    stock_dict[ticker] = yf.Ticker(ticker)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create dictionaries to make dataset building process easier\n",
    "historical_prices = {}\n",
    "quarter_data = {}\n",
    "firm_description = {}\n",
    "\n",
    "filter_names = ['sector', 'longBusinessSummary', 'country', 'industry', 'sharesOutstanding', 'sharesShort']\n",
    "\n",
    "for stock, obj in stock_dict.items():\n",
    "    try:\n",
    "        # share price & volume info\n",
    "        historical_prices[stock] = obj.history(period='16mo', interval='1d')\n",
    "\n",
    "        # quarterly info\n",
    "        qb = obj.quarterly_balancesheet.T\n",
    "        qf = obj.quarterly_financials.T\n",
    "        qc = obj.quarterly_cashflow.T\n",
    "        combined_df = pd.concat((qb, qc, qf), axis=1)\n",
    "        quarter_data[stock] = combined_df\n",
    "\n",
    "        # firm constant info\n",
    "        condensed_info = { name: obj.info[name] for name in filter_names }\n",
    "        firm_description[stock] = pd.DataFrame.from_records(condensed_info, index=[0])\n",
    "        print('stock {} loaded'.format(stock))\n",
    "    except:\n",
    "        print('\\n***problem loading stock : {}***\\n'.format(stock))\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# remove stocks that don't exist for all dictionaries\n",
    "remove_stocks = []\n",
    "for stock_name in historical_prices.keys():\n",
    "    if stock_name not in firm_description:\n",
    "        remove_stocks.append(stock_name)\n",
    "\n",
    "for stock_name in remove_stocks:\n",
    "    del historical_prices[stock_name]\n",
    "\n",
    "remove_stocks = []\n",
    "\n",
    "for stock_name in quarter_data.keys():\n",
    "    if stock_name not in firm_description:\n",
    "        remove_stocks.append(stock_name)\n",
    "\n",
    "for stock_name in remove_stocks:\n",
    "    del quarter_data[stock_name]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# quarter dates for price retrieval\n",
    "stock_quarter_dates = {}\n",
    "inadequate_stocks = []\n",
    "for stock, quarter_df in quarter_data.items():\n",
    "    quarter_dts = quarter_df.index.values\n",
    "    quarter_dts.sort()\n",
    "\n",
    "    # delete stock if oldest price date doesn't go back far enough\n",
    "    if historical_prices[stock].index.values[0] > quarter_dts[0] + np.timedelta64(-90, 'D'):\n",
    "        inadequate_stocks.append(stock)\n",
    "        continue\n",
    "\n",
    "    stock_quarter_dates[stock] = pd.DataFrame()\n",
    "    for date in quarter_dts:\n",
    "        find_date = True\n",
    "        while find_date:\n",
    "            if date in historical_prices[stock].index:\n",
    "                price_dict = historical_prices[stock].loc[date]\n",
    "                price_df = pd.DataFrame.from_dict(price_dict)\n",
    "                prev_df = stock_quarter_dates[stock]\n",
    "                stock_quarter_dates[stock] = prev_df.append(price_df.T)\n",
    "                find_date = False\n",
    "            else:\n",
    "                date += np.timedelta64(-1, 'D')\n",
    "            if date < historical_prices[stock].index.values[0]:\n",
    "                inadequate_stocks.append(stock)\n",
    "                find_date = False\n",
    "                break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for stock in inadequate_stocks:\n",
    "    del quarter_data[stock]\n",
    "    del historical_prices[stock]\n",
    "    del firm_description[stock]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# consolidate into singular df\n",
    "stock_consolidated_dict = {}\n",
    "\n",
    "for stock, quarter_price_df in stock_quarter_dates.items():\n",
    "    # get columns to use\n",
    "    column_list = np.concatenate((np.array(['Date']),stock_quarter_dates[stock].columns.values, quarter_data[stock].columns.values, firm_description[stock].columns.values))\n",
    "    stock_df = pd.DataFrame(index=['2019_Q2', '2019_Q3', '2019_Q4', '2020_Q1'], columns=column_list)\n",
    "    for i, index in enumerate(stock_df.index.values):\n",
    "        append_series = pd.Series(quarter_price_df.index[i], index=['Date']).astype('str')\n",
    "        append_series = append_series.append(quarter_price_df.iloc[i])\n",
    "        append_series = append_series.append(quarter_data[stock].iloc[i])\n",
    "        append_series = append_series.append(firm_description[stock].iloc[0])\n",
    "        stock_df.loc[index] = append_series\n",
    "    stock_consolidated_dict[stock] = stock_df.to_dict(orient='index')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.datetime64):\n",
    "            return str(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# convert dictionaries to json objects; save to directory\n",
    "with open('stock_specific_info.json', 'w') as fp:\n",
    "    json.dump(stock_consolidated_dict, fp, cls=NpEncoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "historical_price_dict = {}\n",
    "for stock in historical_prices:\n",
    "    historical_prices[stock] = historical_prices[stock].loc[~historical_prices[stock].index.duplicated(keep='last')]\n",
    "    historical_price_dict[stock] = historical_prices[stock].to_dict(orient='index')\n",
    "    historical_price_dict[stock] = { str(key): value for key, value in historical_price_dict[stock].items() }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('stock_historic_prices.json', 'w') as fp:\n",
    "    json.dump(historical_price_dict, fp, cls=NpEncoder)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Augment Data with Technical Indicators"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read in .json files\n",
    "with open('stock_specific_info.json', 'r') as fp:\n",
    "    stock_info_dict = json.load(fp)\n",
    "\n",
    "with open('stock_historic_prices.json', 'r') as fp:\n",
    "    stock_prices_dict = json.load(fp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get quarterly returns\n",
    "quarter_returns = pd.read_csv('qtrlyReturns.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# remove no quarter return stocks\n",
    "drop_stocks = []\n",
    "\n",
    "# add quarterly returns to stock_info_dict\n",
    "for stock, quarter_info in stock_info_dict.items():\n",
    "    stock_returns = quarter_returns[quarter_returns.Ticker == stock]\n",
    "    try:\n",
    "        stock_returns[['Q2 2019', 'Q3 2019', 'Q4 2019', 'Q1 2020', '5/21/2020']].astype('float', errors='raise')\n",
    "    except:\n",
    "        print('stock {} has bad qtr return values'.format(stock))\n",
    "        drop_stocks.append(stock)\n",
    "        continue\n",
    "    stock_info_dict[stock]['2019_Q2'].update({ 'Return' : float(stock_returns['Q2 2019'].values[0] ), 'Return Next Quarter' : float(stock_returns['Q3 2019'])})\n",
    "    stock_info_dict[stock]['2019_Q3'].update({ 'Return' : float(stock_returns['Q3 2019'].values[0] ), 'Return Next Quarter' : float(stock_returns['Q4 2019'])})\n",
    "    stock_info_dict[stock]['2019_Q4'].update({ 'Return' : float(stock_returns['Q4 2019'].values[0] ), 'Return Next Quarter' : float(stock_returns['Q1 2020'])})\n",
    "    stock_info_dict[stock]['2020_Q1'].update({ 'Return' : float(stock_returns['Q1 2020'].values[0] ), 'Return Next Quarter' : float(stock_returns['5/21/2020'])})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# delete bad stocks\n",
    "for stock in drop_stocks:\n",
    "    del stock_info_dict[stock]\n",
    "    del stock_prices_dict[stock]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "quarters = ['2019_Q2', '2019_Q3', '2019_Q4', '2020_Q1']\n",
    "\n",
    "# build some techinical indicators from price history for each stock\n",
    "for stock in stock_prices_dict:\n",
    "    # convert to dataframe\n",
    "    price_hist_df = pd.DataFrame.from_records(stock_prices_dict[stock]).T\n",
    "\n",
    "    for qtr in quarters:\n",
    "        # dictionary of indicators to add to stock info\n",
    "        techinical_indicators = {}\n",
    "\n",
    "        # momentum indicators\n",
    "        aoi = ta.momentum.AwesomeOscillatorIndicator(high=price_hist_df.High, low=price_hist_df.Low).ao().loc[str(pd.to_datetime(stock_info_dict[stock][qtr]['Date']))]\n",
    "        techinical_indicators['aoi'] = aoi\n",
    "\n",
    "        kama = ta.momentum.KAMAIndicator(close=price_hist_df.Close).kama().loc[str(pd.to_datetime(stock_info_dict[stock][qtr]['Date']))]\n",
    "        techinical_indicators['kama'] = kama\n",
    "\n",
    "        roc = ta.momentum.ROCIndicator(close=price_hist_df.Close).roc().loc[str(pd.to_datetime(stock_info_dict[stock][qtr]['Date']))]\n",
    "        techinical_indicators['roc'] = roc\n",
    "\n",
    "        rsi = ta.momentum.RSIIndicator(close=price_hist_df.Close).rsi().loc[str(pd.to_datetime(stock_info_dict[stock][qtr]['Date']))]\n",
    "        techinical_indicators['rsi'] = rsi\n",
    "\n",
    "        tsi = ta.momentum.TSIIndicator(close=price_hist_df.Close).tsi().loc[str(pd.to_datetime(stock_info_dict[stock][qtr]['Date']))]\n",
    "        techinical_indicators['tsi'] = tsi\n",
    "\n",
    "        # volume indicators\n",
    "        adi = ta.volume.AccDistIndexIndicator(high=price_hist_df.High, low=price_hist_df.Low, close=price_hist_df.Close, volume=price_hist_df.Volume).acc_dist_index().loc[str(pd.to_datetime(stock_info_dict[stock][qtr]['Date']))]\n",
    "        techinical_indicators['adi'] = adi\n",
    "\n",
    "        cmf = ta.volume.ChaikinMoneyFlowIndicator(high=price_hist_df.High, low=price_hist_df.Low, close=price_hist_df.Close, volume=price_hist_df.Volume).chaikin_money_flow().loc[str(pd.to_datetime(stock_info_dict[stock][qtr]['Date']))]\n",
    "        techinical_indicators['cmf'] = cmf\n",
    "\n",
    "        emi = ta.volume.EaseOfMovementIndicator(high=price_hist_df.High, low=price_hist_df.Low, volume=price_hist_df.Volume).ease_of_movement().loc[str(pd.to_datetime(stock_info_dict[stock][qtr]['Date']))]\n",
    "        techinical_indicators['emi'] = emi\n",
    "\n",
    "\n",
    "        # volatility indicators\n",
    "        atr = ta.volatility.AverageTrueRange(high=price_hist_df.High, low=price_hist_df.Low, close=price_hist_df.Close).average_true_range().loc[str(pd.to_datetime(stock_info_dict[stock][qtr]['Date']))]\n",
    "        techinical_indicators['atr'] = atr\n",
    "\n",
    "        bband_h_indicator = ta.volatility.BollingerBands(close=price_hist_df.Close).bollinger_hband_indicator().loc[str(pd.to_datetime(stock_info_dict[stock][qtr]['Date']))]\n",
    "        techinical_indicators['bband_h'] = bband_h_indicator\n",
    "\n",
    "        bband_l_indicator = ta.volatility.BollingerBands(close=price_hist_df.Close).bollinger_lband_indicator().loc[str(pd.to_datetime(stock_info_dict[stock][qtr]['Date']))]\n",
    "        techinical_indicators['bband_l'] = bband_l_indicator\n",
    "\n",
    "        # trend indicators\n",
    "        adx_pos = ta.trend.ADXIndicator(high=price_hist_df.High, low=price_hist_df.Low, close=price_hist_df.Close).adx_pos().loc[str(pd.to_datetime(stock_info_dict[stock][qtr]['Date']))]\n",
    "        techinical_indicators['adx_pos'] = adx_pos\n",
    "\n",
    "        adx_neg = ta.trend.ADXIndicator(high=price_hist_df.High, low=price_hist_df.Low, close=price_hist_df.Close).adx_neg().loc[str(pd.to_datetime(stock_info_dict[stock][qtr]['Date']))]\n",
    "        techinical_indicators['adx_neg'] = adx_neg\n",
    "\n",
    "        macd = ta.trend.MACD(close=price_hist_df.Close).macd_signal().loc[str(pd.to_datetime(stock_info_dict[stock][qtr]['Date']))]\n",
    "        techinical_indicators['macd'] = macd\n",
    "\n",
    "        # add to exisitng dictionary\n",
    "        stock_info_dict[stock][qtr].update(techinical_indicators)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# turn stock info dictionary into dataframe\n",
    "stock_info_df = pd.DataFrame.from_dict({(i,j): stock_info_dict[i][j]\n",
    "                           for i in stock_info_dict.keys()\n",
    "                           for j in stock_info_dict[i].keys()},\n",
    "                       orient='index')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# combine index\n",
    "stock_info_df = stock_info_df.set_index(stock_info_df.index.get_level_values(0) + ' ' + stock_info_df.index.get_level_values(1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# convert to .csv\n",
    "stock_info_df.to_csv('stock_complete_info.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NLP Augmentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stock_info_df = pd.read_csv('stock_complete_info.csv', index_col=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# do text feature extraction\n",
    "# get bigrams tf_idf\n",
    "\n",
    "company_summary_text = stock_info_df['longBusinessSummary'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# remove special characters\n",
    "def remove_string_special_characters(s):\n",
    "\n",
    "    # removes special characters with ' '\n",
    "    stripped = re.sub('[^a-zA-z\\s]', '', s)\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "\n",
    "    # Change any white space to one space\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "\n",
    "    # Remove start and end white spaces\n",
    "    stripped = stripped.strip()\n",
    "    if stripped != '':\n",
    "            return stripped.lower()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Stopword removal and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "overused_words = ['company', 'founded', 'inc', 'provide', 'formerly', 'known', 'offer', 'also', '']\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for i, line in enumerate(company_summary_text):\n",
    "    line = remove_string_special_characters(line)\n",
    "    company_summary_text[i] = ' '.join([ps.stem(x) for x in word_tokenize(line) if ( x not in stop_words ) and (x not in overused_words)])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tf idf vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range = (2, 2))\n",
    "tf_idf_text = vectorizer.fit_transform(company_summary_text)\n",
    "features = vectorizer.get_feature_names()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Getting top ranking bigrams\n",
    "sums = tf_idf_text.sum(axis = 0)\n",
    "data1 = []\n",
    "for col, term in enumerate(features):\n",
    "    data1.append( (term, sums[0, col] ))\n",
    "ranking = pd.DataFrame(data1, columns = ['term', 'tf_idf'])\n",
    "words = (ranking.sort_values('tf_idf', ascending = True))\n",
    "\n",
    "term_dict = dict.fromkeys(words['term'].values)\n",
    "term_dict = {\" \".join(sorted(key.split(\" \"))):term_dict[key] for key in term_dict}\n",
    "term_rem_dups = list(term_dict.keys())\n",
    "words = words[words['term'].isin(term_rem_dups)]\n",
    "words = (ranking.sort_values('tf_idf', ascending = False))\n",
    "\n",
    "top_500_words = words.head(500)\n",
    "print (\"\\n\\nWords : \\n\", top_500_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# convert to df\n",
    "tf_idf_df = pd.DataFrame(tf_idf_text.toarray(), columns = features)\n",
    "del tf_idf_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# keep only top 500\n",
    "tf_idf_df = tf_idf_df.loc[:, top_500_words['term'].values]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop summary text\n",
    "stock_info_df.drop(columns = 'longBusinessSummary', inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add bigrams as features\n",
    "stock_info_df = stock_info_df.reset_index().join(tf_idf_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set index back\n",
    "stock_info_df = stock_info_df.set_index('index')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# convert to .csv\n",
    "stock_info_df.to_csv('stock_complete_info_bigrams.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import csv\n",
    "csv_file = 'stock_complete_info_bigrams.csv'\n",
    "stock_df = pd.read_csv(csv_file, index_col=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop price / volume related columns\n",
    "stock_df.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop label columns\n",
    "stock_df.drop(columns=['Date', 'Next Qtr', 'Ticker', 'Name'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# data set description\n",
    "stock_df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# see nan's\n",
    "pd.set_option('display.max_rows', 100)\n",
    "stock_df.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop columns with all nan's\n",
    "stock_df.dropna(axis=1, how='all', inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# only keep rows where technical indicators exist\n",
    "stock_df.dropna(axis=0, how='any', subset=['kama', 'cmf', 'emi'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fill nan's with 0's for the rest\n",
    "stock_df.fillna(0, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stock_df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get location of fundamental columns\n",
    "stock_df.columns.get_loc('country')\n",
    "stock_df.columns.get_loc('sharesShort')\n",
    "stock_df.columns.get_loc('Short Term Investments')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get fundamentals on per outstanding share basis\n",
    "fundamentalsPerShare = stock_df.iloc[:,0:62].div(stock_df.sharesOutstanding, axis=0)\n",
    "sharesShortPerShare = stock_df.iloc[:,67:68].div(stock_df.sharesOutstanding, axis=0)\n",
    "extraFundamentalsPerShare = stock_df.iloc[:,83:85].div(stock_df.sharesOutstanding, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transformed dataframe\n",
    "stock_df.drop(columns=['sharesShort', 'Short Term Investments', 'Deferred Long Term Liab'], inplace=True)\n",
    "processed_stock_df = pd.concat([fundamentalsPerShare, sharesShortPerShare, extraFundamentalsPerShare, stock_df.iloc[:,63:]], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "processed_stock_df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# view some data distributions\n",
    "processed_stock_df.iloc[:,:19].boxplot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "processed_stock_df.iloc[:,20:39].boxplot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "processed_stock_df.iloc[:,40:59].boxplot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "processed_stock_df.iloc[:,60:79].boxplot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "outlier_removed_df = processed_stock_df[(np.abs(stats.zscore(processed_stock_df.iloc[:,:86].drop(columns=['country', 'industry', 'sector', 'Return', 'sharesOutstanding']))) < 3).all(axis=1)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# view updated data distributions\n",
    "outlier_removed_df.iloc[:,:19].boxplot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outlier_removed_df.iloc[:,20:39].boxplot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outlier_removed_df.iloc[:,40:59].boxplot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outlier_removed_df.iloc[:,60:79].boxplot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outlier_removed_df.describe(include='object')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get dummies for country, industry, sector\n",
    "outlier_removed_df_dummy = pd.get_dummies(outlier_removed_df, columns=['country', 'industry', 'sector'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save this df as .csv\n",
    "outlier_removed_df_dummy.to_csv('outlier_removed_processed_df_bigrams.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-eb9e9a2d",
   "language": "python",
   "display_name": "PyCharm (DSC478)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}